{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension to reload modules before cell execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test the sigmoid function, and compare it to scipy's implementation to make sure we're getting the right return values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import sigmoid # our sigmoid\n",
    "from scipy.special import expit # scipy's sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these should be the same\n",
    "print(sigmoid(1))\n",
    "print(expit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.linspace(-10,10,100):\n",
    "    assert(sigmoid(i) == expit(i)) # this shouldn't fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sanity check\n",
    "try:\n",
    "    assert(sigmoid(0.7) == expit(0.71)) # this should fail\n",
    "except AssertionError as e:\n",
    "    print(\"It Successfully Failed :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking vector values\n",
    "v = np.array([0.75, 0.3, -0.56, 0.01]) # random vector\n",
    "print(sigmoid(v))\n",
    "print(expit(v))\n",
    "print(\"Are they equal?\", sigmoid(v) == expit(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test the preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\" Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "\n",
    "     Some suggestions for preprocessing step:\n",
    "     - feature selection\"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    # Pick a reasonable size for validation data\n",
    "\n",
    "    # ------------Initialize preprocess arrays----------------------#\n",
    "    train_preprocess = np.zeros(shape=(50000, 784))\n",
    "    validation_preprocess = np.zeros(shape=(10000, 784))\n",
    "    test_preprocess = np.zeros(shape=(10000, 784))\n",
    "    train_label_preprocess = np.zeros(shape=(50000,))\n",
    "    validation_label_preprocess = np.zeros(shape=(10000,))\n",
    "    test_label_preprocess = np.zeros(shape=(10000,))\n",
    "    # ------------Initialize flag variables----------------------#\n",
    "    train_len = 0\n",
    "    validation_len = 0\n",
    "    test_len = 0\n",
    "    train_label_len = 0\n",
    "    validation_label_len = 0\n",
    "    # ------------Start to split the data set into 6 arrays-----------#\n",
    "    for key in mat:\n",
    "        # -----------when the set is training set--------------------#\n",
    "        if \"train\" in key:\n",
    "            label = key[-1]  # record the corresponding label\n",
    "            tup = mat.get(key)\n",
    "            sap = range(tup.shape[0])\n",
    "            tup_perm = np.random.permutation(sap)\n",
    "            tup_len = len(tup)  # get the length of current training set\n",
    "            tag_len = tup_len - 1000  # defines the number of examples which will be added into the training set\n",
    "\n",
    "            # ---------------------adding data to training set-------------------------#\n",
    "            train_preprocess[train_len:train_len + tag_len] = tup[tup_perm[1000:], :]\n",
    "            train_len += tag_len\n",
    "\n",
    "            train_label_preprocess[train_label_len:train_label_len + tag_len] = label\n",
    "            train_label_len += tag_len\n",
    "\n",
    "            # ---------------------adding data to validation set-------------------------#\n",
    "            validation_preprocess[validation_len:validation_len + 1000] = tup[tup_perm[0:1000], :]\n",
    "            validation_len += 1000\n",
    "\n",
    "            validation_label_preprocess[validation_label_len:validation_label_len + 1000] = label\n",
    "            validation_label_len += 1000\n",
    "\n",
    "            # ---------------------adding data to test set-------------------------#\n",
    "        elif \"test\" in key:\n",
    "            label = key[-1]\n",
    "            tup = mat.get(key)\n",
    "            sap = range(tup.shape[0])\n",
    "            tup_perm = np.random.permutation(sap)\n",
    "            tup_len = len(tup)\n",
    "            test_label_preprocess[test_len:test_len + tup_len] = label\n",
    "            test_preprocess[test_len:test_len + tup_len] = tup[tup_perm]\n",
    "            test_len += tup_len\n",
    "            # ---------------------Shuffle,double and normalize-------------------------#\n",
    "    train_size = range(train_preprocess.shape[0])\n",
    "    train_perm = np.random.permutation(train_size)\n",
    "    train_data = train_preprocess[train_perm]\n",
    "    train_data = np.double(train_data)\n",
    "    train_data = train_data / 255.0\n",
    "    train_label = train_label_preprocess[train_perm]\n",
    "\n",
    "    validation_size = range(validation_preprocess.shape[0])\n",
    "    vali_perm = np.random.permutation(validation_size)\n",
    "    validation_data = validation_preprocess[vali_perm]\n",
    "    validation_data = np.double(validation_data)\n",
    "    validation_data = validation_data / 255.0\n",
    "    validation_label = validation_label_preprocess[vali_perm]\n",
    "\n",
    "    test_size = range(test_preprocess.shape[0])\n",
    "    test_perm = np.random.permutation(test_size)\n",
    "    test_data = test_preprocess[test_perm]\n",
    "    test_data = np.double(test_data)\n",
    "    test_data = test_data / 255.0\n",
    "    test_label = test_label_preprocess[test_perm]\n",
    "\n",
    "    # Feature selection\n",
    "    # Your code here.\n",
    "\n",
    "    print('preprocess done')\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now loaded what would normally be the returned data of preprocess() to the above variables.  Now we want to figure out a way to check if a value is the same accross all rows for a given column.\n",
    "\n",
    "Since the algorthm will only be trained on 'train_data', we should only have to test this on that set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/14859458/how-to-check-if-all-values-in-the-columns-of-a-numpy-matrix-are-the-same\n",
    "(train_data == train_data[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(len(res)) # 784 == 28 x 28\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a vector of size 784 indicating `True` if the column is the same for all rows and `False` otherwise.  So, any column that is `True` here is giving us the same value accross the every training example.\n",
    "\n",
    "To make sure we're doing this correctly, we'll perform a similar check in a more intuitive, but disgusting inefficient way.  We see in the above example that the first entry is `True`, meaning that every row should share the same value (either `1` or `0`).\n",
    "\n",
    "So, we'll loop through every example and check if an entry is the same in every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "# 0th Entry is True\n",
    "t0 = train_data[0][0]\n",
    "for t in train_data:\n",
    "    if (t0 != t[0]):\n",
    "        print(\"Something's wrong\") # shouldn't print\n",
    "        \n",
    "# 100th Entry is True\n",
    "t0 = train_data[0][100]\n",
    "for t in train_data:\n",
    "    if (t0 != t[100]):\n",
    "        print(\"This one's right\") # should print\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To see the sum of these values\n",
    "for i in range(784):\n",
    "    s = 0\n",
    "    for t in train_data:\n",
    "        s += t[i]\n",
    "\n",
    "    print(i, res[i], s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the above function works to find these useless features.  We now only need to remove those features from the data sets and note the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "removable_indices = np.where(res)\n",
    "print(removable_indices) # indices of useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_data = np.delete(train_data, removable_indices, 1)\n",
    "print(train_data.shape)\n",
    "print(clean_train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we removed columns from the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(clean_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_train_data = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(res_train_data)\n",
    "\n",
    "res_clean_data = np.all(clean_train_data == clean_train_data[0,:], axis = 0)\n",
    "print(res_clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(res_train_data))\n",
    "print(sum(res_clean_data)) #should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement this into the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(sum(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it seems to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnObjFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import preprocess, sigmoid, initializeWeights\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-22 16:26:08,321 - INFO - preprocess start\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/logging/__init__.py\", line 992, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/logging/__init__.py\", line 838, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/logging/__init__.py\", line 575, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/logging/__init__.py\", line 338, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-99cba232304f>\", line 1, in <module>\n",
      "    train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
      "  File \"/home/nathan/CSE574/project_1/nnScript.py\", line 148, in preprocess\n",
      "    logger.debug(\"Used Features\", used_indices)\n",
      "Message: 'Used Features'\n",
      "Arguments: ((array([ 12,  13,  14,  15,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
      "        41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  58,  59,\n",
      "        60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,\n",
      "        73,  74,  75,  76,  77,  78,  79,  80,  81,  86,  87,  88,  89,\n",
      "        90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
      "       103, 104, 105, 106, 107, 108, 109, 110, 113, 114, 115, 116, 117,\n",
      "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
      "       131, 132, 133, 134, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
      "       146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158,\n",
      "       159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172,\n",
      "       173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185,\n",
      "       186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
      "       199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "       212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
      "       225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
      "       251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263,\n",
      "       264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276,\n",
      "       277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289,\n",
      "       290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302,\n",
      "       303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
      "       316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328,\n",
      "       329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341,\n",
      "       342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354,\n",
      "       355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "       368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380,\n",
      "       381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 393, 394,\n",
      "       395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407,\n",
      "       408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,\n",
      "       421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "       434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446,\n",
      "       447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459,\n",
      "       460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472,\n",
      "       473, 474, 475, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 561, 562, 563, 564, 565,\n",
      "       566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578,\n",
      "       579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591,\n",
      "       592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604,\n",
      "       605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617,\n",
      "       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
      "       631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643,\n",
      "       646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658,\n",
      "       659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 674,\n",
      "       675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687,\n",
      "       688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 702, 703,\n",
      "       704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716,\n",
      "       717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 731, 732, 733,\n",
      "       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
      "       747, 748, 749, 750, 751, 752, 753, 760, 761, 762, 763, 764, 765,\n",
      "       766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778,\n",
      "       779]),),)\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/logging/__init__.py\", line 992, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/logging/__init__.py\", line 838, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/logging/__init__.py\", line 575, in format\n",
      "    record.message = record.getMessage()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/logging/__init__.py\", line 338, in getMessage\n",
      "    msg = msg % self.args\n",
      "TypeError: not all arguments converted during string formatting\n",
      "Call stack:\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n",
      "    app.start()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 478, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n",
      "    super(ZMQIOLoop, self).start()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/tornado/ioloop.py\", line 888, in start\n",
      "    handler_func(fd_obj, events)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 233, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 208, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 537, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2728, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2850, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/nathan/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2910, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-4-99cba232304f>\", line 1, in <module>\n",
      "    train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
      "  File \"/home/nathan/CSE574/project_1/nnScript.py\", line 148, in preprocess\n",
      "    logger.debug(\"Used Features\", used_indices)\n",
      "Message: 'Used Features'\n",
      "Arguments: ((array([ 12,  13,  14,  15,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
      "        41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  58,  59,\n",
      "        60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,\n",
      "        73,  74,  75,  76,  77,  78,  79,  80,  81,  86,  87,  88,  89,\n",
      "        90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
      "       103, 104, 105, 106, 107, 108, 109, 110, 113, 114, 115, 116, 117,\n",
      "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
      "       131, 132, 133, 134, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
      "       146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158,\n",
      "       159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172,\n",
      "       173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185,\n",
      "       186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
      "       199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "       212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
      "       225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
      "       251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263,\n",
      "       264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276,\n",
      "       277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289,\n",
      "       290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302,\n",
      "       303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
      "       316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328,\n",
      "       329, 330, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341,\n",
      "       342, 343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354,\n",
      "       355, 356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367,\n",
      "       368, 369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380,\n",
      "       381, 382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 393, 394,\n",
      "       395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407,\n",
      "       408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,\n",
      "       421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "       434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446,\n",
      "       447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459,\n",
      "       460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472,\n",
      "       473, 474, 475, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 561, 562, 563, 564, 565,\n",
      "       566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578,\n",
      "       579, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 590, 591,\n",
      "       592, 593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604,\n",
      "       605, 606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 616, 617,\n",
      "       618, 619, 620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630,\n",
      "       631, 632, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643,\n",
      "       646, 647, 648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658,\n",
      "       659, 660, 661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 674,\n",
      "       675, 676, 677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687,\n",
      "       688, 689, 690, 691, 692, 693, 694, 695, 696, 697, 698, 702, 703,\n",
      "       704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716,\n",
      "       717, 718, 719, 720, 721, 722, 723, 724, 725, 726, 731, 732, 733,\n",
      "       734, 735, 736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746,\n",
      "       747, 748, 749, 750, 751, 752, 753, 760, 761, 762, 763, 764, 765,\n",
      "       766, 767, 768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778,\n",
      "       779]),),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-22 16:26:13,731 - INFO - preprocess complete.\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "#  Train Neural Network\n",
    "\n",
    "# set the number of nodes in input unit (not including bias unit)\n",
    "n_input = train_data.shape[1]\n",
    "\n",
    "# set the number of nodes in hidden unit (not including bias unit)\n",
    "n_hidden = 50\n",
    "\n",
    "# set the number of nodes in output unit\n",
    "n_class = 10\n",
    "\n",
    "# initialize the weights into some random matrices\n",
    "initial_w1 = initializeWeights(n_input, n_hidden)\n",
    "initial_w2 = initializeWeights(n_hidden, n_class)\n",
    "\n",
    "# unroll 2 weight matrices into single column vector\n",
    "initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()), 0)\n",
    "\n",
    "# set the regularization hyper-parameter\n",
    "lambdaval = 0\n",
    "\n",
    "args = (n_input, n_hidden, n_class, train_data, train_label, lambdaval)\n",
    "\n",
    "# Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example\n",
    "\n",
    "opts = {'maxiter': 50}  # Preferred value.\n",
    "\n",
    "#nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnObjFunction(params, *args):\n",
    "    n_input, n_hidden, n_class, training_data, training_label, lambdaval = args\n",
    "    \n",
    "#     training_data = training_data[:7000]\n",
    "#     training_label = training_label[:7000]\n",
    "\n",
    "    w1 = params[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "    w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "    obj_val = 0\n",
    "\n",
    "    # Your code here\n",
    "    \n",
    "    # Set Bias\n",
    "    \n",
    "    b1 = np.ones((len(training_data), 1))\n",
    "    b2 = np.ones((len(training_data), 1))\n",
    "\n",
    "    # Forward Propagation\n",
    "    X = np.append(training_data, b1, 1) # append bias\n",
    "    net1 = X.dot(w1.T)\n",
    "    o1 = sigmoid(net1)\n",
    "    \n",
    "    H = np.append(o1, b2, 1)\n",
    "    net2 = H.dot(w2.T)\n",
    "    o2 = sigmoid(net2)\n",
    "    \n",
    "    # 1-hot encoding\n",
    "    y = np.zeros(o2.shape)\n",
    "    y[np.arange(o2.shape[0]), training_label.astype(int)] = 1\n",
    "    \n",
    "    # Error\n",
    "    E = (y*np.log(o2) + (np.ones(y.shape) - y)*np.log(np.ones(o2.shape) - o2))\n",
    "    obj_val = -(np.sum(E) / len(training_data))\n",
    "    \n",
    "    plt_data.append(obj_val)\n",
    "    \n",
    "    # Gradients\n",
    "    grad_w2 = np.dot((o2-y).T, H)\n",
    "    sm = (o2 - y).dot(w2[:,:-1]).T # note: we remove the bias from w2\n",
    "    tm = ((1-o1)*o1).T\n",
    "    grad_w1 = (sm * tm).dot(X)\n",
    "    \n",
    "    # Make sure you reshape the gradient matrices to a 1D array. for instance if your gradient matrices are grad_w1 and grad_w2\n",
    "    # you would use code similar to the one below to create a flat array\n",
    "    obj_grad = np.concatenate((grad_w1.flatten(), grad_w2.flatten()),0)\n",
    "    #obj_grad = np.array([])\n",
    "\n",
    "    return (obj_val, obj_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Iter: 1\n",
      "Iter: 2\n",
      "Iter: 3\n",
      "Iter: 4\n",
      "Iter: 5\n",
      "Iter: 6\n",
      "Iter: 7\n",
      "Iter: 8\n",
      "Iter: 9\n",
      "Iter: 10\n",
      "Iter: 11\n",
      "Iter: 12\n",
      "Iter: 13\n",
      "Iter: 14\n",
      "Iter: 15\n",
      "Iter: 16\n",
      "Iter: 17\n",
      "Iter: 18\n",
      "Iter: 19\n",
      "Iter: 20\n",
      "Iter: 21\n",
      "Iter: 22\n",
      "Iter: 23\n",
      "Iter: 24\n",
      "Iter: 25\n",
      "Iter: 26\n",
      "Iter: 27\n",
      "Iter: 28\n",
      "Iter: 29\n",
      "Iter: 30\n",
      "Iter: 31\n",
      "Iter: 32\n",
      "Iter: 33\n",
      "Iter: 34\n",
      "Iter: 35\n",
      "Iter: 36\n",
      "Iter: 37\n",
      "Iter: 38\n",
      "Iter: 39\n",
      "Iter: 40\n",
      "Iter: 41\n",
      "Iter: 42\n",
      "Iter: 43\n",
      "Iter: 44\n",
      "Iter: 45\n",
      "Iter: 46\n",
      "Iter: 47\n",
      "Iter: 48\n",
      "Iter: 49\n"
     ]
    }
   ],
   "source": [
    "plt_data = []\n",
    "opts = {'maxiter': 10}\n",
    "iter_weights = initialWeights\n",
    "for i in range(50):\n",
    "    print(\"Iter: {}\".format(i))\n",
    "    random_sample_index = np.random.choice(np.arange(len(train_data)), 1000)\n",
    "    args = (n_input, n_hidden, n_class, train_data[random_sample_index], train_label[random_sample_index], lambdaval)\n",
    "    nn_params = minimize(nnObjFunction, iter_weights, jac=True, args=args, method='CG', options=opts)\n",
    "    iter_weights = nn_params.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.016724335764854322\n",
       "     jac: array([0.        , 0.        , 0.        , ..., 0.25478014, 0.06498164,\n",
       "       0.12753736])\n",
       " message: 'Maximum number of iterations has been exceeded.'\n",
       "    nfev: 59\n",
       "     nit: 10\n",
       "    njev: 59\n",
       "  status: 1\n",
       " success: False\n",
       "       x: array([-0.08312907, -0.07581697,  0.06730041, ...,  1.99525665,\n",
       "       -3.66318158, -1.79328317])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3cf14719b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(plt_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot ought to show a pretty successful learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to \"reverse\" the process and use our newly optimized weights to predict a label given a feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredict(w1, w2, data):\n",
    "    \"\"\"% nnPredict predicts the label of data given the parameter w1, w2 of Neural\n",
    "    % Network.\n",
    "\n",
    "    % Input:\n",
    "    % w1: matrix of weights of connections from input layer to hidden layers.\n",
    "    %     w1(i, j) represents the weight of connection from unit i in input \n",
    "    %     layer to unit j in hidden layer.\n",
    "    % w2: matrix of weights of connections from hidden layer to output layers.\n",
    "    %     w2(i, j) represents the weight of connection from unit i in input \n",
    "    %     layer to unit j in hidden layer.\n",
    "    % data: matrix of data. Each row of this matrix represents the feature \n",
    "    %       vector of a particular image\n",
    "       \n",
    "    % Output: \n",
    "    % label: a column vector of predicted labels\"\"\"\n",
    "\n",
    "    labels = np.array([])\n",
    "    bias = np.ones((len(data), 1))\n",
    "\n",
    "    # Forward Propagation\n",
    "    X = np.append(data, bias, 1) # append bias\n",
    "    net1 = X.dot(w1.T)\n",
    "    o1 = sigmoid(net1)\n",
    "    \n",
    "    H = np.append(o1, bias, 1)\n",
    "    net2 = H.dot(w2.T)\n",
    "    o2 = sigmoid(net2)\n",
    "    \n",
    "    labels = np.array(np.argmax(o2, axis=1))\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = nn_params.x[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "w2 = nn_params.x[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "\n",
    "# Test the computed parameters\n",
    "\n",
    "predicted_label = nnPredict(w1, w2, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 2 4 ... 7 0 8]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 2. 4. ... 7. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.916 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", sum(predicted_label == train_label)/len(train_label)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
