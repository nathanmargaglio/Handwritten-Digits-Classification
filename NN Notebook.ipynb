{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension to reload modules before cell execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test the sigmoid function, and compare it to scipy's implementation to make sure we're getting the right return values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import sigmoid # our sigmoid\n",
    "from scipy.special import expit # scipy's sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these should be the same\n",
    "print(sigmoid(1))\n",
    "print(expit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.linspace(-10,10,100):\n",
    "    assert(sigmoid(i) == expit(i)) # this shouldn't fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sanity check\n",
    "try:\n",
    "    assert(sigmoid(0.7) == expit(0.71)) # this should fail\n",
    "except AssertionError as e:\n",
    "    print(\"It Successfully Failed :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking vector values\n",
    "v = np.array([0.75, 0.3, -0.56, 0.01]) # random vector\n",
    "print(sigmoid(v))\n",
    "print(expit(v))\n",
    "print(\"Are they equal?\", sigmoid(v) == expit(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test the preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\" Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "\n",
    "     Some suggestions for preprocessing step:\n",
    "     - feature selection\"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    # Pick a reasonable size for validation data\n",
    "\n",
    "    # ------------Initialize preprocess arrays----------------------#\n",
    "    train_preprocess = np.zeros(shape=(50000, 784))\n",
    "    validation_preprocess = np.zeros(shape=(10000, 784))\n",
    "    test_preprocess = np.zeros(shape=(10000, 784))\n",
    "    train_label_preprocess = np.zeros(shape=(50000,))\n",
    "    validation_label_preprocess = np.zeros(shape=(10000,))\n",
    "    test_label_preprocess = np.zeros(shape=(10000,))\n",
    "    # ------------Initialize flag variables----------------------#\n",
    "    train_len = 0\n",
    "    validation_len = 0\n",
    "    test_len = 0\n",
    "    train_label_len = 0\n",
    "    validation_label_len = 0\n",
    "    # ------------Start to split the data set into 6 arrays-----------#\n",
    "    for key in mat:\n",
    "        # -----------when the set is training set--------------------#\n",
    "        if \"train\" in key:\n",
    "            label = key[-1]  # record the corresponding label\n",
    "            tup = mat.get(key)\n",
    "            sap = range(tup.shape[0])\n",
    "            tup_perm = np.random.permutation(sap)\n",
    "            tup_len = len(tup)  # get the length of current training set\n",
    "            tag_len = tup_len - 1000  # defines the number of examples which will be added into the training set\n",
    "\n",
    "            # ---------------------adding data to training set-------------------------#\n",
    "            train_preprocess[train_len:train_len + tag_len] = tup[tup_perm[1000:], :]\n",
    "            train_len += tag_len\n",
    "\n",
    "            train_label_preprocess[train_label_len:train_label_len + tag_len] = label\n",
    "            train_label_len += tag_len\n",
    "\n",
    "            # ---------------------adding data to validation set-------------------------#\n",
    "            validation_preprocess[validation_len:validation_len + 1000] = tup[tup_perm[0:1000], :]\n",
    "            validation_len += 1000\n",
    "\n",
    "            validation_label_preprocess[validation_label_len:validation_label_len + 1000] = label\n",
    "            validation_label_len += 1000\n",
    "\n",
    "            # ---------------------adding data to test set-------------------------#\n",
    "        elif \"test\" in key:\n",
    "            label = key[-1]\n",
    "            tup = mat.get(key)\n",
    "            sap = range(tup.shape[0])\n",
    "            tup_perm = np.random.permutation(sap)\n",
    "            tup_len = len(tup)\n",
    "            test_label_preprocess[test_len:test_len + tup_len] = label\n",
    "            test_preprocess[test_len:test_len + tup_len] = tup[tup_perm]\n",
    "            test_len += tup_len\n",
    "            # ---------------------Shuffle,double and normalize-------------------------#\n",
    "    train_size = range(train_preprocess.shape[0])\n",
    "    train_perm = np.random.permutation(train_size)\n",
    "    train_data = train_preprocess[train_perm]\n",
    "    train_data = np.double(train_data)\n",
    "    train_data = train_data / 255.0\n",
    "    train_label = train_label_preprocess[train_perm]\n",
    "\n",
    "    validation_size = range(validation_preprocess.shape[0])\n",
    "    vali_perm = np.random.permutation(validation_size)\n",
    "    validation_data = validation_preprocess[vali_perm]\n",
    "    validation_data = np.double(validation_data)\n",
    "    validation_data = validation_data / 255.0\n",
    "    validation_label = validation_label_preprocess[vali_perm]\n",
    "\n",
    "    test_size = range(test_preprocess.shape[0])\n",
    "    test_perm = np.random.permutation(test_size)\n",
    "    test_data = test_preprocess[test_perm]\n",
    "    test_data = np.double(test_data)\n",
    "    test_data = test_data / 255.0\n",
    "    test_label = test_label_preprocess[test_perm]\n",
    "\n",
    "    # Feature selection\n",
    "    # Your code here.\n",
    "\n",
    "    print('preprocess done')\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now loaded what would normally be the returned data of preprocess() to the above variables.  Now we want to figure out a way to check if a value is the same accross all rows for a given column.\n",
    "\n",
    "Since the algorthm will only be trained on 'train_data', we should only have to test this on that set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/14859458/how-to-check-if-all-values-in-the-columns-of-a-numpy-matrix-are-the-same\n",
    "(train_data == train_data[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(len(res)) # 784 == 28 x 28\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a vector of size 784 indicating `True` if the column is the same for all rows and `False` otherwise.  So, any column that is `True` here is giving us the same value accross the every training example.\n",
    "\n",
    "To make sure we're doing this correctly, we'll perform a similar check in a more intuitive, but disgusting inefficient way.  We see in the above example that the first entry is `True`, meaning that every row should share the same value (either `1` or `0`).\n",
    "\n",
    "So, we'll loop through every example and check if an entry is the same in every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "# 0th Entry is True\n",
    "t0 = train_data[0][0]\n",
    "for t in train_data:\n",
    "    if (t0 != t[0]):\n",
    "        print(\"Something's wrong\") # shouldn't print\n",
    "        \n",
    "# 100th Entry is True\n",
    "t0 = train_data[0][100]\n",
    "for t in train_data:\n",
    "    if (t0 != t[100]):\n",
    "        print(\"This one's right\") # should print\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To see the sum of these values\n",
    "for i in range(784):\n",
    "    s = 0\n",
    "    for t in train_data:\n",
    "        s += t[i]\n",
    "\n",
    "    print(i, res[i], s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the above function works to find these useless features.  We now only need to remove those features from the data sets and note the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "removable_indices = np.where(res)\n",
    "print(removable_indices) # indices of useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_data = np.delete(train_data, removable_indices, 1)\n",
    "print(train_data.shape)\n",
    "print(clean_train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we removed columns from the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(clean_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_train_data = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(res_train_data)\n",
    "\n",
    "res_clean_data = np.all(clean_train_data == clean_train_data[0,:], axis = 0)\n",
    "print(res_clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(res_train_data))\n",
    "print(sum(res_clean_data)) #should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement this into the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(sum(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it seems to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnObjFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import preprocess, sigmoid, initializeWeights\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Features (array([ 12,  13,  14,  15,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
      "        41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  58,  59,\n",
      "        60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,\n",
      "        73,  74,  75,  76,  77,  78,  79,  80,  81,  86,  87,  88,  89,\n",
      "        90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
      "       103, 104, 105, 106, 107, 108, 109, 110, 113, 114, 115, 116, 117,\n",
      "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
      "       131, 132, 133, 134, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
      "       146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158,\n",
      "       159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172,\n",
      "       173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185,\n",
      "       186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
      "       199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "       212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
      "       225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
      "       251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263,\n",
      "       264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276,\n",
      "       277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289,\n",
      "       290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302,\n",
      "       303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
      "       316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328,\n",
      "       329, 330, 331, 332, 333, 334, 335, 337, 338, 339, 340, 341, 342,\n",
      "       343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
      "       356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368,\n",
      "       369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "       382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394,\n",
      "       395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407,\n",
      "       408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,\n",
      "       421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "       434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446,\n",
      "       447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459,\n",
      "       460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472,\n",
      "       473, 474, 475, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 561, 562, 563, 564, 565,\n",
      "       566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578,\n",
      "       579, 580, 581, 582, 583, 584, 585, 586, 587, 589, 590, 591, 592,\n",
      "       593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605,\n",
      "       606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 617, 618, 619,\n",
      "       620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632,\n",
      "       633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 646, 647,\n",
      "       648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660,\n",
      "       661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 674, 675, 676,\n",
      "       677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689,\n",
      "       690, 691, 692, 693, 694, 695, 696, 697, 698, 702, 703, 704, 705,\n",
      "       706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718,\n",
      "       719, 720, 721, 722, 723, 724, 725, 726, 731, 732, 733, 734, 735,\n",
      "       736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748,\n",
      "       749, 750, 751, 752, 753, 760, 761, 762, 763, 764, 765, 766, 767,\n",
      "       768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779]),)\n",
      "preprocess done\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "#  Train Neural Network\n",
    "\n",
    "# set the number of nodes in input unit (not including bias unit)\n",
    "n_input = train_data.shape[1]\n",
    "\n",
    "# set the number of nodes in hidden unit (not including bias unit)\n",
    "n_hidden = 50\n",
    "\n",
    "# set the number of nodes in output unit\n",
    "n_class = 10\n",
    "\n",
    "# initialize the weights into some random matrices\n",
    "initial_w1 = initializeWeights(n_input, n_hidden)\n",
    "initial_w2 = initializeWeights(n_hidden, n_class)\n",
    "\n",
    "# unroll 2 weight matrices into single column vector\n",
    "initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()), 0)\n",
    "\n",
    "# set the regularization hyper-parameter\n",
    "lambdaval = 0\n",
    "\n",
    "args = (n_input, n_hidden, n_class, train_data, train_label, lambdaval)\n",
    "\n",
    "# Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example\n",
    "\n",
    "opts = {'maxiter': 50}  # Preferred value.\n",
    "\n",
    "#nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnObjFunction(params, *args):\n",
    "    n_input, n_hidden, n_class, training_data, training_label, lambdaval = args\n",
    "    \n",
    "#     training_data = training_data[:7000]\n",
    "#     training_label = training_label[:7000]\n",
    "\n",
    "    w1 = params[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "    w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "    obj_val = 0\n",
    "\n",
    "    # Your code here\n",
    "    \n",
    "    # Set Bias\n",
    "    \n",
    "    b1 = np.ones((len(training_data), 1))\n",
    "    b2 = np.ones((len(training_data), 1))\n",
    "\n",
    "    # Forward Propagation\n",
    "    X = np.append(training_data, b1, 1) # append bias\n",
    "    net1 = X.dot(w1.T)\n",
    "    o1 = sigmoid(net1)\n",
    "    \n",
    "    H = np.append(o1, b2, 1)\n",
    "    net2 = H.dot(w2.T)\n",
    "    o2 = sigmoid(net2)\n",
    "    \n",
    "    # 1-hot encoding\n",
    "    y = np.zeros(o2.shape)\n",
    "    y[np.arange(o2.shape[0]), training_label.astype(int)] = 1\n",
    "    \n",
    "    # Error\n",
    "    E = (y*np.log(o2) + (np.ones(y.shape) - y)*np.log(np.ones(o2.shape) - o2))\n",
    "    obj_val = -(np.sum(E) / len(training_data))\n",
    "    \n",
    "    plt_data.append(obj_val)\n",
    "    \n",
    "    # Gradients\n",
    "    grad_w2 = np.dot((o2-y).T, H)\n",
    "    sm = (o2 - y).dot(w2[:,:-1]).T # note: we remove the bias from w2\n",
    "    tm = ((1-o1)*o1).T\n",
    "    grad_w1 = (sm * tm).dot(X)\n",
    "    \n",
    "    # Make sure you reshape the gradient matrices to a 1D array. for instance if your gradient matrices are grad_w1 and grad_w2\n",
    "    # you would use code similar to the one below to create a flat array\n",
    "    obj_grad = np.concatenate((grad_w1.flatten(), grad_w2.flatten()),0)\n",
    "    #obj_grad = np.array([])\n",
    "\n",
    "    return (obj_val, obj_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter: 0\n",
      "Iter: 1\n",
      "Iter: 2\n",
      "Iter: 3\n",
      "Iter: 4\n",
      "Iter: 5\n",
      "Iter: 6\n",
      "Iter: 7\n",
      "Iter: 8\n",
      "Iter: 9\n",
      "Iter: 10\n",
      "Iter: 11\n",
      "Iter: 12\n",
      "Iter: 13\n",
      "Iter: 14\n",
      "Iter: 15\n",
      "Iter: 16\n",
      "Iter: 17\n",
      "Iter: 18\n",
      "Iter: 19\n",
      "Iter: 20\n",
      "Iter: 21\n",
      "Iter: 22\n",
      "Iter: 23\n",
      "Iter: 24\n",
      "Iter: 25\n",
      "Iter: 26\n",
      "Iter: 27\n",
      "Iter: 28\n",
      "Iter: 29\n",
      "Iter: 30\n",
      "Iter: 31\n",
      "Iter: 32\n",
      "Iter: 33\n",
      "Iter: 34\n",
      "Iter: 35\n",
      "Iter: 36\n",
      "Iter: 37\n",
      "Iter: 38\n",
      "Iter: 39\n",
      "Iter: 40\n",
      "Iter: 41\n",
      "Iter: 42\n",
      "Iter: 43\n",
      "Iter: 44\n",
      "Iter: 45\n",
      "Iter: 46\n",
      "Iter: 47\n",
      "Iter: 48\n",
      "Iter: 49\n",
      "Iter: 50\n",
      "Iter: 51\n",
      "Iter: 52\n",
      "Iter: 53\n",
      "Iter: 54\n",
      "Iter: 55\n",
      "Iter: 56\n",
      "Iter: 57\n",
      "Iter: 58\n",
      "Iter: 59\n",
      "Iter: 60\n",
      "Iter: 61\n",
      "Iter: 62\n",
      "Iter: 63\n",
      "Iter: 64\n",
      "Iter: 65\n",
      "Iter: 66\n",
      "Iter: 67\n",
      "Iter: 68\n",
      "Iter: 69\n",
      "Iter: 70\n",
      "Iter: 71\n",
      "Iter: 72\n",
      "Iter: 73\n",
      "Iter: 74\n",
      "Iter: 75\n",
      "Iter: 76\n",
      "Iter: 77\n",
      "Iter: 78\n",
      "Iter: 79\n",
      "Iter: 80\n",
      "Iter: 81\n",
      "Iter: 82\n",
      "Iter: 83\n",
      "Iter: 84\n",
      "Iter: 85\n",
      "Iter: 86\n",
      "Iter: 87\n",
      "Iter: 88\n",
      "Iter: 89\n",
      "Iter: 90\n",
      "Iter: 91\n",
      "Iter: 92\n",
      "Iter: 93\n",
      "Iter: 94\n",
      "Iter: 95\n",
      "Iter: 96\n",
      "Iter: 97\n",
      "Iter: 98\n",
      "Iter: 99\n",
      "Iter: 100\n",
      "Iter: 101\n",
      "Iter: 102\n",
      "Iter: 103\n",
      "Iter: 104\n",
      "Iter: 105\n",
      "Iter: 106\n",
      "Iter: 107\n",
      "Iter: 108\n",
      "Iter: 109\n",
      "Iter: 110\n",
      "Iter: 111\n",
      "Iter: 112\n",
      "Iter: 113\n",
      "Iter: 114\n",
      "Iter: 115\n",
      "Iter: 116\n",
      "Iter: 117\n",
      "Iter: 118\n",
      "Iter: 119\n",
      "Iter: 120\n",
      "Iter: 121\n",
      "Iter: 122\n",
      "Iter: 123\n",
      "Iter: 124\n",
      "Iter: 125\n",
      "Iter: 126\n",
      "Iter: 127\n",
      "Iter: 128\n",
      "Iter: 129\n",
      "Iter: 130\n",
      "Iter: 131\n",
      "Iter: 132\n",
      "Iter: 133\n",
      "Iter: 134\n",
      "Iter: 135\n",
      "Iter: 136\n",
      "Iter: 137\n",
      "Iter: 138\n",
      "Iter: 139\n",
      "Iter: 140\n",
      "Iter: 141\n",
      "Iter: 142\n",
      "Iter: 143\n",
      "Iter: 144\n",
      "Iter: 145\n",
      "Iter: 146\n",
      "Iter: 147\n",
      "Iter: 148\n",
      "Iter: 149\n",
      "Iter: 150\n",
      "Iter: 151\n",
      "Iter: 152\n",
      "Iter: 153\n",
      "Iter: 154\n",
      "Iter: 155\n",
      "Iter: 156\n",
      "Iter: 157\n",
      "Iter: 158\n",
      "Iter: 159\n",
      "Iter: 160\n",
      "Iter: 161\n",
      "Iter: 162\n",
      "Iter: 163\n",
      "Iter: 164\n",
      "Iter: 165\n",
      "Iter: 166\n",
      "Iter: 167\n",
      "Iter: 168\n",
      "Iter: 169\n",
      "Iter: 170\n",
      "Iter: 171\n",
      "Iter: 172\n",
      "Iter: 173\n",
      "Iter: 174\n",
      "Iter: 175\n",
      "Iter: 176\n",
      "Iter: 177\n",
      "Iter: 178\n",
      "Iter: 179\n",
      "Iter: 180\n",
      "Iter: 181\n",
      "Iter: 182\n",
      "Iter: 183\n",
      "Iter: 184\n",
      "Iter: 185\n",
      "Iter: 186\n",
      "Iter: 187\n",
      "Iter: 188\n",
      "Iter: 189\n",
      "Iter: 190\n",
      "Iter: 191\n",
      "Iter: 192\n",
      "Iter: 193\n",
      "Iter: 194\n",
      "Iter: 195\n",
      "Iter: 196\n",
      "Iter: 197\n",
      "Iter: 198\n",
      "Iter: 199\n",
      "Iter: 200\n",
      "Iter: 201\n",
      "Iter: 202\n",
      "Iter: 203\n",
      "Iter: 204\n",
      "Iter: 205\n",
      "Iter: 206\n",
      "Iter: 207\n",
      "Iter: 208\n",
      "Iter: 209\n",
      "Iter: 210\n",
      "Iter: 211\n",
      "Iter: 212\n",
      "Iter: 213\n",
      "Iter: 214\n",
      "Iter: 215\n",
      "Iter: 216\n",
      "Iter: 217\n",
      "Iter: 218\n",
      "Iter: 219\n",
      "Iter: 220\n",
      "Iter: 221\n",
      "Iter: 222\n",
      "Iter: 223\n",
      "Iter: 224\n",
      "Iter: 225\n",
      "Iter: 226\n",
      "Iter: 227\n",
      "Iter: 228\n",
      "Iter: 229\n",
      "Iter: 230\n",
      "Iter: 231\n",
      "Iter: 232\n",
      "Iter: 233\n",
      "Iter: 234\n",
      "Iter: 235\n",
      "Iter: 236\n",
      "Iter: 237\n",
      "Iter: 238\n",
      "Iter: 239\n",
      "Iter: 240\n",
      "Iter: 241\n",
      "Iter: 242\n",
      "Iter: 243\n",
      "Iter: 244\n",
      "Iter: 245\n",
      "Iter: 246\n",
      "Iter: 247\n",
      "Iter: 248\n",
      "Iter: 249\n",
      "Iter: 250\n",
      "Iter: 251\n",
      "Iter: 252\n",
      "Iter: 253\n",
      "Iter: 254\n",
      "Iter: 255\n",
      "Iter: 256\n",
      "Iter: 257\n",
      "Iter: 258\n",
      "Iter: 259\n",
      "Iter: 260\n",
      "Iter: 261\n",
      "Iter: 262\n",
      "Iter: 263\n",
      "Iter: 264\n",
      "Iter: 265\n",
      "Iter: 266\n",
      "Iter: 267\n",
      "Iter: 268\n",
      "Iter: 269\n",
      "Iter: 270\n",
      "Iter: 271\n",
      "Iter: 272\n",
      "Iter: 273\n",
      "Iter: 274\n",
      "Iter: 275\n",
      "Iter: 276\n",
      "Iter: 277\n",
      "Iter: 278\n",
      "Iter: 279\n",
      "Iter: 280\n",
      "Iter: 281\n",
      "Iter: 282\n",
      "Iter: 283\n",
      "Iter: 284\n",
      "Iter: 285\n",
      "Iter: 286\n",
      "Iter: 287\n",
      "Iter: 288\n",
      "Iter: 289\n",
      "Iter: 290\n",
      "Iter: 291\n",
      "Iter: 292\n",
      "Iter: 293\n",
      "Iter: 294\n",
      "Iter: 295\n",
      "Iter: 296\n",
      "Iter: 297\n",
      "Iter: 298\n",
      "Iter: 299\n",
      "Iter: 300\n",
      "Iter: 301\n",
      "Iter: 302\n",
      "Iter: 303\n",
      "Iter: 304\n",
      "Iter: 305\n",
      "Iter: 306\n",
      "Iter: 307\n",
      "Iter: 308\n",
      "Iter: 309\n",
      "Iter: 310\n",
      "Iter: 311\n",
      "Iter: 312\n",
      "Iter: 313\n",
      "Iter: 314\n",
      "Iter: 315\n",
      "Iter: 316\n",
      "Iter: 317\n",
      "Iter: 318\n",
      "Iter: 319\n",
      "Iter: 320\n",
      "Iter: 321\n",
      "Iter: 322\n",
      "Iter: 323\n",
      "Iter: 324\n",
      "Iter: 325\n",
      "Iter: 326\n",
      "Iter: 327\n",
      "Iter: 328\n",
      "Iter: 329\n",
      "Iter: 330\n",
      "Iter: 331\n",
      "Iter: 332\n",
      "Iter: 333\n",
      "Iter: 334\n",
      "Iter: 335\n",
      "Iter: 336\n",
      "Iter: 337\n",
      "Iter: 338\n",
      "Iter: 339\n",
      "Iter: 340\n",
      "Iter: 341\n",
      "Iter: 342\n",
      "Iter: 343\n",
      "Iter: 344\n",
      "Iter: 345\n",
      "Iter: 346\n",
      "Iter: 347\n",
      "Iter: 348\n",
      "Iter: 349\n",
      "Iter: 350\n",
      "Iter: 351\n",
      "Iter: 352\n",
      "Iter: 353\n",
      "Iter: 354\n",
      "Iter: 355\n",
      "Iter: 356\n",
      "Iter: 357\n",
      "Iter: 358\n",
      "Iter: 359\n",
      "Iter: 360\n",
      "Iter: 361\n",
      "Iter: 362\n",
      "Iter: 363\n",
      "Iter: 364\n",
      "Iter: 365\n",
      "Iter: 366\n",
      "Iter: 367\n",
      "Iter: 368\n",
      "Iter: 369\n",
      "Iter: 370\n",
      "Iter: 371\n",
      "Iter: 372\n",
      "Iter: 373\n",
      "Iter: 374\n",
      "Iter: 375\n",
      "Iter: 376\n",
      "Iter: 377\n",
      "Iter: 378\n",
      "Iter: 379\n",
      "Iter: 380\n",
      "Iter: 381\n",
      "Iter: 382\n",
      "Iter: 383\n",
      "Iter: 384\n",
      "Iter: 385\n",
      "Iter: 386\n",
      "Iter: 387\n",
      "Iter: 388\n",
      "Iter: 389\n",
      "Iter: 390\n",
      "Iter: 391\n",
      "Iter: 392\n",
      "Iter: 393\n",
      "Iter: 394\n",
      "Iter: 395\n",
      "Iter: 396\n",
      "Iter: 397\n",
      "Iter: 398\n",
      "Iter: 399\n",
      "Iter: 400\n",
      "Iter: 401\n",
      "Iter: 402\n",
      "Iter: 403\n",
      "Iter: 404\n",
      "Iter: 405\n",
      "Iter: 406\n",
      "Iter: 407\n",
      "Iter: 408\n",
      "Iter: 409\n",
      "Iter: 410\n",
      "Iter: 411\n",
      "Iter: 412\n",
      "Iter: 413\n",
      "Iter: 414\n",
      "Iter: 415\n",
      "Iter: 416\n",
      "Iter: 417\n",
      "Iter: 418\n",
      "Iter: 419\n",
      "Iter: 420\n",
      "Iter: 421\n",
      "Iter: 422\n",
      "Iter: 423\n",
      "Iter: 424\n",
      "Iter: 425\n",
      "Iter: 426\n",
      "Iter: 427\n",
      "Iter: 428\n",
      "Iter: 429\n",
      "Iter: 430\n",
      "Iter: 431\n",
      "Iter: 432\n",
      "Iter: 433\n",
      "Iter: 434\n",
      "Iter: 435\n",
      "Iter: 436\n",
      "Iter: 437\n",
      "Iter: 438\n",
      "Iter: 439\n",
      "Iter: 440\n",
      "Iter: 441\n",
      "Iter: 442\n",
      "Iter: 443\n",
      "Iter: 444\n",
      "Iter: 445\n",
      "Iter: 446\n",
      "Iter: 447\n",
      "Iter: 448\n",
      "Iter: 449\n",
      "Iter: 450\n",
      "Iter: 451\n",
      "Iter: 452\n",
      "Iter: 453\n",
      "Iter: 454\n",
      "Iter: 455\n",
      "Iter: 456\n",
      "Iter: 457\n",
      "Iter: 458\n",
      "Iter: 459\n",
      "Iter: 460\n",
      "Iter: 461\n",
      "Iter: 462\n",
      "Iter: 463\n",
      "Iter: 464\n",
      "Iter: 465\n",
      "Iter: 466\n",
      "Iter: 467\n",
      "Iter: 468\n",
      "Iter: 469\n",
      "Iter: 470\n",
      "Iter: 471\n",
      "Iter: 472\n",
      "Iter: 473\n",
      "Iter: 474\n",
      "Iter: 475\n",
      "Iter: 476\n",
      "Iter: 477\n",
      "Iter: 478\n",
      "Iter: 479\n",
      "Iter: 480\n",
      "Iter: 481\n",
      "Iter: 482\n",
      "Iter: 483\n",
      "Iter: 484\n",
      "Iter: 485\n",
      "Iter: 486\n",
      "Iter: 487\n",
      "Iter: 488\n",
      "Iter: 489\n",
      "Iter: 490\n",
      "Iter: 491\n",
      "Iter: 492\n",
      "Iter: 493\n",
      "Iter: 494\n",
      "Iter: 495\n",
      "Iter: 496\n",
      "Iter: 497\n",
      "Iter: 498\n",
      "Iter: 499\n"
     ]
    }
   ],
   "source": [
    "plt_data = []\n",
    "opts = {'maxiter': 1}\n",
    "iter_weights = initialWeights\n",
    "for i in range(500):\n",
    "    print(\"Iter: {}\".format(i))\n",
    "    random_sample_index = np.random.choice(np.arange(len(train_data)), 1000)\n",
    "    args = (n_input, n_hidden, n_class, train_data[random_sample_index], train_label[random_sample_index], lambdaval)\n",
    "    nn_params = minimize(nnObjFunction, iter_weights, jac=True, args=args, method='CG', options=opts)\n",
    "    iter_weights = nn_params.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.3591468439109258\n",
       "     jac: array([ 0.        ,  0.        ,  0.        , ...,  0.4143018 ,\n",
       "       -0.0896678 , -4.01734185])\n",
       " message: 'Maximum number of iterations has been exceeded.'\n",
       "    nfev: 3\n",
       "     nit: 1\n",
       "    njev: 3\n",
       "  status: 1\n",
       " success: False\n",
       "       x: array([-0.08312873, -0.07580961,  0.06730777, ..., -1.13913475,\n",
       "       -2.33783022, -0.56240824])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3cf13065f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(plt_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot ought to show a pretty successful learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to \"reverse\" the process and use our newly optimized weights to predict a label given a feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredict(w1, w2, data):\n",
    "    \"\"\"% nnPredict predicts the label of data given the parameter w1, w2 of Neural\n",
    "    % Network.\n",
    "\n",
    "    % Input:\n",
    "    % w1: matrix of weights of connections from input layer to hidden layers.\n",
    "    %     w1(i, j) represents the weight of connection from unit i in input \n",
    "    %     layer to unit j in hidden layer.\n",
    "    % w2: matrix of weights of connections from hidden layer to output layers.\n",
    "    %     w2(i, j) represents the weight of connection from unit i in input \n",
    "    %     layer to unit j in hidden layer.\n",
    "    % data: matrix of data. Each row of this matrix represents the feature \n",
    "    %       vector of a particular image\n",
    "       \n",
    "    % Output: \n",
    "    % label: a column vector of predicted labels\"\"\"\n",
    "\n",
    "    labels = np.array([])\n",
    "    bias = np.ones((len(data), 1))\n",
    "\n",
    "    # Forward Propagation\n",
    "    X = np.append(data, bias, 1) # append bias\n",
    "    net1 = X.dot(w1.T)\n",
    "    o1 = sigmoid(net1)\n",
    "    \n",
    "    H = np.append(o1, bias, 1)\n",
    "    net2 = H.dot(w2.T)\n",
    "    o2 = sigmoid(net2)\n",
    "    \n",
    "    labels = np.array(np.argmax(o2, axis=1))\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = nn_params.x[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "w2 = nn_params.x[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "\n",
    "# Test the computed parameters\n",
    "\n",
    "predicted_label = nnPredict(w1, w2, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 2 4 ... 7 0 8]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 2. 4. ... 7. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.916 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", sum(predicted_label == train_label)/len(train_label)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
