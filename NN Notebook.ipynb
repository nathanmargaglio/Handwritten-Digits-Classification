{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extension to reload modules before cell execution\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll test the sigmoid function, and compare it to scipy's implementation to make sure we're getting the right return values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import sigmoid # our sigmoid\n",
    "from scipy.special import expit # scipy's sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these should be the same\n",
    "print(sigmoid(1))\n",
    "print(expit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.linspace(-10,10,100):\n",
    "    assert(sigmoid(i) == expit(i)) # this shouldn't fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for sanity check\n",
    "try:\n",
    "    assert(sigmoid(0.7) == expit(0.71)) # this should fail\n",
    "except AssertionError as e:\n",
    "    print(\"It Successfully Failed :D\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking vector values\n",
    "v = np.array([0.75, 0.3, -0.56, 0.01]) # random vector\n",
    "print(sigmoid(v))\n",
    "print(expit(v))\n",
    "print(\"Are they equal?\", sigmoid(v) == expit(v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll test the preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "    \"\"\" Input:\n",
    "     Although this function doesn't have any input, you are required to load\n",
    "     the MNIST data set from file 'mnist_all.mat'.\n",
    "\n",
    "     Output:\n",
    "     train_data: matrix of training set. Each row of train_data contains \n",
    "       feature vector of a image\n",
    "     train_label: vector of label corresponding to each image in the training\n",
    "       set\n",
    "     validation_data: matrix of training set. Each row of validation_data \n",
    "       contains feature vector of a image\n",
    "     validation_label: vector of label corresponding to each image in the \n",
    "       training set\n",
    "     test_data: matrix of training set. Each row of test_data contains \n",
    "       feature vector of a image\n",
    "     test_label: vector of label corresponding to each image in the testing\n",
    "       set\n",
    "\n",
    "     Some suggestions for preprocessing step:\n",
    "     - feature selection\"\"\"\n",
    "\n",
    "    mat = loadmat('mnist_all.mat')  # loads the MAT object as a Dictionary\n",
    "\n",
    "    # Pick a reasonable size for validation data\n",
    "\n",
    "    # ------------Initialize preprocess arrays----------------------#\n",
    "    train_preprocess = np.zeros(shape=(50000, 784))\n",
    "    validation_preprocess = np.zeros(shape=(10000, 784))\n",
    "    test_preprocess = np.zeros(shape=(10000, 784))\n",
    "    train_label_preprocess = np.zeros(shape=(50000,))\n",
    "    validation_label_preprocess = np.zeros(shape=(10000,))\n",
    "    test_label_preprocess = np.zeros(shape=(10000,))\n",
    "    # ------------Initialize flag variables----------------------#\n",
    "    train_len = 0\n",
    "    validation_len = 0\n",
    "    test_len = 0\n",
    "    train_label_len = 0\n",
    "    validation_label_len = 0\n",
    "    # ------------Start to split the data set into 6 arrays-----------#\n",
    "    for key in mat:\n",
    "        # -----------when the set is training set--------------------#\n",
    "        if \"train\" in key:\n",
    "            label = key[-1]  # record the corresponding label\n",
    "            tup = mat.get(key)\n",
    "            sap = range(tup.shape[0])\n",
    "            tup_perm = np.random.permutation(sap)\n",
    "            tup_len = len(tup)  # get the length of current training set\n",
    "            tag_len = tup_len - 1000  # defines the number of examples which will be added into the training set\n",
    "\n",
    "            # ---------------------adding data to training set-------------------------#\n",
    "            train_preprocess[train_len:train_len + tag_len] = tup[tup_perm[1000:], :]\n",
    "            train_len += tag_len\n",
    "\n",
    "            train_label_preprocess[train_label_len:train_label_len + tag_len] = label\n",
    "            train_label_len += tag_len\n",
    "\n",
    "            # ---------------------adding data to validation set-------------------------#\n",
    "            validation_preprocess[validation_len:validation_len + 1000] = tup[tup_perm[0:1000], :]\n",
    "            validation_len += 1000\n",
    "\n",
    "            validation_label_preprocess[validation_label_len:validation_label_len + 1000] = label\n",
    "            validation_label_len += 1000\n",
    "\n",
    "            # ---------------------adding data to test set-------------------------#\n",
    "        elif \"test\" in key:\n",
    "            label = key[-1]\n",
    "            tup = mat.get(key)\n",
    "            sap = range(tup.shape[0])\n",
    "            tup_perm = np.random.permutation(sap)\n",
    "            tup_len = len(tup)\n",
    "            test_label_preprocess[test_len:test_len + tup_len] = label\n",
    "            test_preprocess[test_len:test_len + tup_len] = tup[tup_perm]\n",
    "            test_len += tup_len\n",
    "            # ---------------------Shuffle,double and normalize-------------------------#\n",
    "    train_size = range(train_preprocess.shape[0])\n",
    "    train_perm = np.random.permutation(train_size)\n",
    "    train_data = train_preprocess[train_perm]\n",
    "    train_data = np.double(train_data)\n",
    "    train_data = train_data / 255.0\n",
    "    train_label = train_label_preprocess[train_perm]\n",
    "\n",
    "    validation_size = range(validation_preprocess.shape[0])\n",
    "    vali_perm = np.random.permutation(validation_size)\n",
    "    validation_data = validation_preprocess[vali_perm]\n",
    "    validation_data = np.double(validation_data)\n",
    "    validation_data = validation_data / 255.0\n",
    "    validation_label = validation_label_preprocess[vali_perm]\n",
    "\n",
    "    test_size = range(test_preprocess.shape[0])\n",
    "    test_perm = np.random.permutation(test_size)\n",
    "    test_data = test_preprocess[test_perm]\n",
    "    test_data = np.double(test_data)\n",
    "    test_data = test_data / 255.0\n",
    "    test_label = test_label_preprocess[test_perm]\n",
    "\n",
    "    # Feature selection\n",
    "    # Your code here.\n",
    "\n",
    "    print('preprocess done')\n",
    "\n",
    "    return train_data, train_label, validation_data, validation_label, test_data, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now loaded what would normally be the returned data of preprocess() to the above variables.  Now we want to figure out a way to check if a value is the same accross all rows for a given column.\n",
    "\n",
    "Since the algorthm will only be trained on 'train_data', we should only have to test this on that set of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#https://stackoverflow.com/questions/14859458/how-to-check-if-all-values-in-the-columns-of-a-numpy-matrix-are-the-same\n",
    "(train_data == train_data[0,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(len(res)) # 784 == 28 x 28\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above is a vector of size 784 indicating `True` if the column is the same for all rows and `False` otherwise.  So, any column that is `True` here is giving us the same value accross the every training example.\n",
    "\n",
    "To make sure we're doing this correctly, we'll perform a similar check in a more intuitive, but disgusting inefficient way.  We see in the above example that the first entry is `True`, meaning that every row should share the same value (either `1` or `0`).\n",
    "\n",
    "So, we'll loop through every example and check if an entry is the same in every row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "\n",
    "# 0th Entry is True\n",
    "t0 = train_data[0][0]\n",
    "for t in train_data:\n",
    "    if (t0 != t[0]):\n",
    "        print(\"Something's wrong\") # shouldn't print\n",
    "        \n",
    "# 100th Entry is True\n",
    "t0 = train_data[0][100]\n",
    "for t in train_data:\n",
    "    if (t0 != t[100]):\n",
    "        print(\"This one's right\") # should print\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# To see the sum of these values\n",
    "for i in range(784):\n",
    "    s = 0\n",
    "    for t in train_data:\n",
    "        s += t[i]\n",
    "\n",
    "    print(i, res[i], s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the above function works to find these useless features.  We now only need to remove those features from the data sets and note the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "removable_indices = np.where(res)\n",
    "print(removable_indices) # indices of useless features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_data = np.delete(train_data, removable_indices, 1)\n",
    "print(train_data.shape)\n",
    "print(clean_train_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how we removed columns from the matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data.shape)\n",
    "print(clean_train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_train_data = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(res_train_data)\n",
    "\n",
    "res_clean_data = np.all(clean_train_data == clean_train_data[0,:], axis = 0)\n",
    "print(res_clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(res_train_data))\n",
    "print(sum(res_clean_data)) #should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we implement this into the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = np.all(train_data == train_data[0,:], axis = 0)\n",
    "print(sum(res))\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And it seems to work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnObjFunction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nnScript import preprocess, sigmoid, initializeWeights\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Used Features (array([ 12,  13,  14,  15,  32,  33,  34,  35,  36,  37,  38,  39,  40,\n",
      "        41,  42,  43,  44,  45,  46,  47,  48,  49,  50,  51,  58,  59,\n",
      "        60,  61,  62,  63,  64,  65,  66,  67,  68,  69,  70,  71,  72,\n",
      "        73,  74,  75,  76,  77,  78,  79,  80,  81,  86,  87,  88,  89,\n",
      "        90,  91,  92,  93,  94,  95,  96,  97,  98,  99, 100, 101, 102,\n",
      "       103, 104, 105, 106, 107, 108, 109, 110, 113, 114, 115, 116, 117,\n",
      "       118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130,\n",
      "       131, 132, 133, 134, 135, 136, 137, 138, 139, 142, 143, 144, 145,\n",
      "       146, 147, 148, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158,\n",
      "       159, 160, 161, 162, 163, 164, 165, 166, 167, 169, 170, 171, 172,\n",
      "       173, 174, 175, 176, 177, 178, 179, 180, 181, 182, 183, 184, 185,\n",
      "       186, 187, 188, 189, 190, 191, 192, 193, 194, 195, 196, 197, 198,\n",
      "       199, 200, 201, 202, 203, 204, 205, 206, 207, 208, 209, 210, 211,\n",
      "       212, 213, 214, 215, 216, 217, 218, 219, 220, 221, 222, 223, 224,\n",
      "       225, 226, 227, 228, 229, 230, 231, 232, 233, 234, 235, 236, 237,\n",
      "       238, 239, 240, 241, 242, 243, 244, 245, 246, 247, 248, 249, 250,\n",
      "       251, 252, 253, 254, 255, 256, 257, 258, 259, 260, 261, 262, 263,\n",
      "       264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 274, 275, 276,\n",
      "       277, 278, 279, 280, 281, 282, 283, 284, 285, 286, 287, 288, 289,\n",
      "       290, 291, 292, 293, 294, 295, 296, 297, 298, 299, 300, 301, 302,\n",
      "       303, 304, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315,\n",
      "       316, 317, 318, 319, 320, 321, 322, 323, 324, 325, 326, 327, 328,\n",
      "       329, 330, 331, 332, 333, 334, 335, 337, 338, 339, 340, 341, 342,\n",
      "       343, 344, 345, 346, 347, 348, 349, 350, 351, 352, 353, 354, 355,\n",
      "       356, 357, 358, 359, 360, 361, 362, 363, 364, 365, 366, 367, 368,\n",
      "       369, 370, 371, 372, 373, 374, 375, 376, 377, 378, 379, 380, 381,\n",
      "       382, 383, 384, 385, 386, 387, 388, 389, 390, 391, 392, 393, 394,\n",
      "       395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407,\n",
      "       408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420,\n",
      "       421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433,\n",
      "       434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446,\n",
      "       447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459,\n",
      "       460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472,\n",
      "       473, 474, 475, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486,\n",
      "       487, 488, 489, 490, 491, 492, 493, 494, 495, 496, 497, 498, 499,\n",
      "       500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512,\n",
      "       513, 514, 515, 516, 517, 518, 519, 520, 521, 522, 523, 524, 525,\n",
      "       526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538,\n",
      "       539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551,\n",
      "       552, 553, 554, 555, 556, 557, 558, 559, 561, 562, 563, 564, 565,\n",
      "       566, 567, 568, 569, 570, 571, 572, 573, 574, 575, 576, 577, 578,\n",
      "       579, 580, 581, 582, 583, 584, 585, 586, 587, 589, 590, 591, 592,\n",
      "       593, 594, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605,\n",
      "       606, 607, 608, 609, 610, 611, 612, 613, 614, 615, 617, 618, 619,\n",
      "       620, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632,\n",
      "       633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 643, 646, 647,\n",
      "       648, 649, 650, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660,\n",
      "       661, 662, 663, 664, 665, 666, 667, 668, 669, 670, 674, 675, 676,\n",
      "       677, 678, 679, 680, 681, 682, 683, 684, 685, 686, 687, 688, 689,\n",
      "       690, 691, 692, 693, 694, 695, 696, 697, 698, 702, 703, 704, 705,\n",
      "       706, 707, 708, 709, 710, 711, 712, 713, 714, 715, 716, 717, 718,\n",
      "       719, 720, 721, 722, 723, 724, 725, 726, 731, 732, 733, 734, 735,\n",
      "       736, 737, 738, 739, 740, 741, 742, 743, 744, 745, 746, 747, 748,\n",
      "       749, 750, 751, 752, 753, 760, 761, 762, 763, 764, 765, 766, 767,\n",
      "       768, 769, 770, 771, 772, 773, 774, 775, 776, 777, 778, 779]),)\n",
      "preprocess done\n"
     ]
    }
   ],
   "source": [
    "train_data, train_label, validation_data, validation_label, test_data, test_label = preprocess()\n",
    "\n",
    "#  Train Neural Network\n",
    "\n",
    "# set the number of nodes in input unit (not including bias unit)\n",
    "n_input = train_data.shape[1]\n",
    "\n",
    "# set the number of nodes in hidden unit (not including bias unit)\n",
    "n_hidden = 50\n",
    "\n",
    "# set the number of nodes in output unit\n",
    "n_class = 10\n",
    "\n",
    "# initialize the weights into some random matrices\n",
    "initial_w1 = initializeWeights(n_input, n_hidden)\n",
    "initial_w2 = initializeWeights(n_hidden, n_class)\n",
    "\n",
    "# unroll 2 weight matrices into single column vector\n",
    "initialWeights = np.concatenate((initial_w1.flatten(), initial_w2.flatten()), 0)\n",
    "\n",
    "# set the regularization hyper-parameter\n",
    "lambdaval = 0\n",
    "\n",
    "args = (n_input, n_hidden, n_class, train_data, train_label, lambdaval)\n",
    "\n",
    "# Train Neural Network using fmin_cg or minimize from scipy,optimize module. Check documentation for a working example\n",
    "\n",
    "opts = {'maxiter': 50}  # Preferred value.\n",
    "\n",
    "#nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnObjFunction(params, *args):\n",
    "    n_input, n_hidden, n_class, training_data, training_label, lambdaval = args\n",
    "    \n",
    "    training_data = training_data[:5000]\n",
    "    training_label = training_label[:5000]\n",
    "\n",
    "    w1 = params[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "    w2 = params[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "    obj_val = 0\n",
    "\n",
    "    # Your code here\n",
    "    \n",
    "    # Set Bias\n",
    "    \n",
    "    b1 = np.ones((len(training_data), 1))\n",
    "    b2 = np.ones((len(training_data), 1))\n",
    "\n",
    "    # Forward Propagation\n",
    "    X = np.append(training_data, b1, 1) # append bias\n",
    "    net1 = X.dot(w1.T)\n",
    "    o1 = sigmoid(net1)\n",
    "    \n",
    "    H = np.append(o1, b2, 1)\n",
    "    net2 = H.dot(w2.T)\n",
    "    o2 = sigmoid(net2)\n",
    "    \n",
    "    # 1-hot encoding\n",
    "    y = np.zeros(o2.shape)\n",
    "    y[np.arange(o2.shape[0]), training_label.astype(int)] = 1\n",
    "    \n",
    "    # Error\n",
    "    E = (y*np.log(o2) + (np.ones(y.shape) - y)*np.log(np.ones(o2.shape) - o2))\n",
    "    obj_val = -(np.sum(E) / len(training_data))\n",
    "    \n",
    "    plt_data.append(obj_val)\n",
    "    \n",
    "    # Gradients\n",
    "    grad_w2 = np.dot((o2-y).T, H)\n",
    "    sm = (o2 - y).dot(w2[:,:-1]).T # note: we remove the bias from w2\n",
    "    tm = ((1-o1)*o1).T\n",
    "    grad_w1 = (sm * tm).dot(X)\n",
    "    \n",
    "    # Make sure you reshape the gradient matrices to a 1D array. for instance if your gradient matrices are grad_w1 and grad_w2\n",
    "    # you would use code similar to the one below to create a flat array\n",
    "    obj_grad = np.concatenate((grad_w1.flatten(), grad_w2.flatten()),0)\n",
    "    #obj_grad = np.array([])\n",
    "\n",
    "    return (obj_val, obj_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt_data = []\n",
    "nn_params = minimize(nnObjFunction, initialWeights, jac=True, args=args, method='CG', options=opts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     fun: 0.13627530905408775\n",
       "     jac: array([ 0.        ,  0.        ,  0.        , ...,  8.51067204,\n",
       "        3.49507374, 14.00993989])\n",
       " message: 'Maximum number of iterations has been exceeded.'\n",
       "    nfev: 395\n",
       "     nit: 50\n",
       "    njev: 395\n",
       "  status: 1\n",
       " success: False\n",
       "       x: array([-0.08312907, -0.07581697,  0.06730041, ..., -1.38046869,\n",
       "       -2.15670208, -0.82972247])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAW4AAAD8CAYAAABXe05zAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvNQv5yAAAHeJJREFUeJzt3Xl0XOWZ5/Hvc2uTSpIt2ZKFsfEWL0BMAkQsCSShgaSBTkIyyfShk87S3Rmf7iyd9PRMOpmcCd2nmzndnck+WdrZpyGQDoF0wiQBEmBIJolBNgYMxmC8YYMteZVtWVItz/xRV7KMpVLZ6Fbdkn6fc3RUdeta9fgV/PzWe9/7vubuiIhI/QhqXYCIiJwaBbeISJ1RcIuI1BkFt4hInVFwi4jUGQW3iEidUXCLiNQZBbeISJ1RcIuI1JlkFD+0vb3dFy1aFMWPFhGZktauXbvX3TsqOTeS4F60aBHd3d1R/GgRkSnJzLZXem5FQyVm9ldm9oSZbTCzW82s4fTLExGRl2LC4DazecBfAl3uvhJIADdEXZiIiIyt0ouTSaDRzJJAFng+upJERKScCYPb3XcB/xPYAbwAHHL3e6IuTERExlbJUEkbcD2wGDgTaDKzPx7jvFVm1m1m3b29vZNfqYiIAJUNlVwNbHX3XnfPAXcAr3nxSe6+2t273L2ro6OiGS0iInIaKgnuHcClZpY1MwOuAjZGW5aIiIxnwnnc7r7GzG4H1gF54BFgdRTFfPGXz5BMGO999SKaMpFMMRcRqXsVpaO73wjcGHEtfPbepwFYNqeFN5zbGfXbiYjUpVitVfKzj7wWgEKxWONKRETiK1bBHZgBUNTG8yIi44pZcJe+F13JLSIynlgFt6nHLSIyoVgF93CP29XjFhEZV8yCe7jHreAWERlPPINbk0pERMYVq+A2XZwUEZlQrII7CAe5ldsiIuOLV3Crxy0iMqGYBXcpuQsKbhGRccUquI+Pcde2DhGROItVcCdseIxbyS0iMp5YBffx6YAKbhGR8cQzuJXbIiLjilVwW1iNZpWIiIwvVsEdmOZxi4hMpJJd3leY2fpRX31m9tFIitE8bhGRCVWy5+Qm4HwAM0sAu4A7oyhGY9wiIhM71aGSq4Bn3X17FMVorRIRkYmdanDfANw61gtmtsrMus2su7e39/SK0TxuEZEJVRzcZpYG3gL8YKzX3X21u3e5e1dHR8fpFaOhEhGRCZ1Kj/taYJ2774msGA2ViIhM6FSC+48YZ5hksmjPSRGRiVUU3GaWBd4A3BFtOaVet8a4RUTGN+F0QAB37wdmR1wLUBrn1lCJiMj4YnXnJAwHd62rEBGJr9gFt5kuToqIlBO74A7MtFaJiEgZMQxurcctIlJODINbY9wiIuXELrg1xi0iUl7sgjsITPO4RUTKiF9wa6hERKSsGAa3hkpERMqJXXCb7pwUESkrdsGdMKNYrHUVIiLxFbvg1lCJiEh5sQtu08VJEZGyYhfcQaBlXUVEyolfcOvipIhIWTEN7lpXISISX5XugNNqZreb2VNmttHMXh1VQbrlXUSkvIp2wAG+APzc3d8R7vaejaogLesqIlLehMFtZjOA1wHvA3D3IWAoqoI0HVBEpLxKhkqWAL3At83sETP7hpk1RVaQLk6KiJRVSXAngQuBr7r7BcBR4OMvPsnMVplZt5l19/b2nnZBmsctIlJeJcG9E9jp7mvC57dTCvITuPtqd+9y966Ojo7TL8g0j1tEpJwJg9vddwPPmdmK8NBVwJORFaQet4hIWZXOKvkwcEs4o2QL8CdRFaSLkyIi5VUU3O6+HuiKuBZAY9wiIhOJ4Z2TGuMWESknhsGt6YAiIuXEM7i1kYKIyLhiF9xaq0REpLzYBbfWKhERKS9+wR2oxy0iUk78glsXJ0VEyopdcGset4hIebELbt05KSJSXuyCO6GhEhGRsmIX3KZ53CIiZcUuuDVUIiJSXgyDW/O4RUTKiV9wax63iEhZsQtu08VJEZGyYhfcGioRESkvhsGtoRIRkXIq2gHHzLYBh4ECkHf3yHbD0Z6TIiLlVbrnJMDvufveyCoJaVlXEZHyYjhUojFuEZFyKg1uB+4xs7VmtirSgtTjFhEpq9Khksvc/XkzmwPca2ZPufuDo08IA30VwIIFC067IC3rKiJSXkU9bnd/PvzeA9wJXDzGOavdvcvduzo6Ok67IC3rKiJS3oTBbWZNZtYy/Bh4I7AhsoIMXD1uEZFxVTJU0gncaWbD53/P3X8eVUGaDigiUt6Ewe3uW4BXVqEWQBcnRUQmErvpgKX1uBXcIiLjiV1wax63iEh5MQxuDZWIiJQTv+AOdHFSRKSc2AW31ioRESkvdsGtMW4RkfJiGNxQUHKLiIwrhsGttUpERMqJZXC767Z3EZHxxDK4AY1zi4iMI4bBXfqu4RIRkbHFL7jD5NZcbhGRscUuuE09bhGRsmIX3BrjFhEpL4bBXfquHreIyNhiGNzDY9wKbhGRscQuuM10cVJEpJyKg9vMEmb2iJndFWlB4VCJbsARERnbqfS4PwJsjKqQYcNDJbes2UHP4YGo305EpO5UFNxmNh/4A+Ab0ZYDi9qbMINP372J677wKzb3HIn6LUVE6kqlPe7PAx8DihHWAsDrl3ew+abruOvDl3NkMM+//nZb1G8pIlJXJgxuM3sT0OPuayc4b5WZdZtZd29v70sqKhEYK+fN5PXLO/j5E7u1ebCIyCiV9LgvA95iZtuA24ArzezmF5/k7qvdvcvduzo6OialuKvO6WRP3yCbezVcIiIybMLgdvdPuPt8d18E3ADc5+5/HHllwMJZWQD29OkipYjIsNjN4x5tzowGAHr6BmtciYhIfCRP5WR3fwB4IJJKxjCnJQNAz2EFt4jIsFj3uJsySbLpBL0KbhGREbEObij1unUjjojIcbEP7o6WjHrcIiKjxD6457Q0KLhFREaJfXCrxy0icqLYB3drNsXhwTz5QuR324uI1IXYB/fMxhQAfQP5GlciIhIPsQ/u1mwpuA/2D9W4EhGReIh/cDemATh0LFfjSkRE4iH2wT1zuMet4BYRAeohuMMx7kP9Cm4RETjFtUpqoTUM7o9+fz03/XQjLZkkr37ZbP7r76+gNZuucXUiItUX++Ae7nEDnH1GC9l0gu8//BwH+of4yrteVcPKRERqI/bBnUwcH8258c0vZ+mcZj5zzya+dN9mNvccZumclhpWJyJSfbEf4x5tSXsTAO97zSICgx+s3cmBo5omKCLTS+x73KMFgQEwuznDK89q5V/+7xZue+g5/unt59GcSdGUSXDO3Bk0pBI1rlREJDp1Edy//OvXk06c+OHgmpefwSM7DpIvFPnzm9eNHO+ckeHr7+niFfNbq12miEhVmHv5HdTNrAF4EMhQCvrb3f3Gcn+mq6vLu7u7J63IseQKRbb0HqVzRoate4+SLzp7Dw/yqR8/wYrOFm5+/yWRvr+IyGQys7Xu3lXJuZX0uAeBK939iJmlgF+b2c/c/XcvqcqXKJUIWHFG6cLkBQuOTwvcsvcon757E2u3H2B+WyOd4b6VIiJTRSW7vLu7HwmfpsKv8t30Grr6nE4A3v7V33DJ//hljasREZl8Fc0qMbOEma0HeoB73X3NGOesMrNuM+vu7e2d7DorNr+tsWbvLSJSDRUFt7sX3P18YD5wsZmtHOOc1e7e5e5dHR0dk11nxZoyyRNu2tE63iIy1ZzSPG53Pwg8AFwTSTWTJBlOGwTYr3neIjLFTBjcZtZhZq3h40bgauCpqAt7KQqjZsr0aNszEZliKplVMhf4rpklKAX9v7n7XdGW9dK0NCQ5GK4m2HtEwS0iU8uEwe3ujwEXVKGWSfPN917EV+7fzI/WP6+NhkVkyqmrtUoqtbyzhX98+ysA2Lm/XxcoRWRKqYtb3k9HQypBWzbFF+/bzBfv20w2neC8eTO56W0rtaKgiNS1KRvcAN9630U8tvMQB/qHONif48ePPs8Hb3mEn3z4ctLJKflhQ0SmgSkd3BcsaOOCBW0jz7sWtfGh7z3CQ1v3c/my9hpWJiJy+qZVt/PypaWw3vD8oRpXIiJy+qZVcLdm08xrbWTDLgW3iNSvaRXcACvnzeDJ5/tqXYaIyGmbdsG9vLOFrfuOUijGdoFDEZGypl1wz25K4w4H+7WGiYjUp2kX3LOaM4AWnxKR+jXtgnt2U2m3nH0KbhGpU9MuuGeFwa0et4jUq2kX3Opxi0i9m3bB3TYc3FruVUTq1LQL7lQiYEZDUkMlIlK3pl1wA7Q3Z9i69yhP7znMAQW4iNSZSrYuO8vM7jezjWb2hJl9pBqFRenM1kZ+9cxe3vi5B+m66Rd8+u5Y78QmInKCSlYHzAN/7e7rzKwFWGtm97r7kxHXFpnP33A+G3Yd4shgnruf2MOX73+WK1bM4aJFs2pdmojIhCrZuuwF4IXw8WEz2wjMA+o2uNubM1yxYg4AV549h98+u5fv/GabgltE6sIpjXGb2SJK+0+uiaKYWsimk7x2WQdrtuzDXeuXiEj8VRzcZtYM/BD4qLuftLyema0ys24z6+7t7Z3MGiP36iWz2XtkiG/+eiv/vn4Xv9m8l5z2qRSRmKpoBxwzS1EK7Vvc/Y6xznH31cBqgK6urrrqul6+rJ1UwviH/7Nx5NjZZ7Rw63+6dGTet4hIXEwY3GZmwDeBje7+2ehLqr4zWxt5+JNX03csz1ChyOO7DvJX33+UWx/ewQeuWFrr8kRETlDJUMllwLuBK81sffh1XcR1VV1rNs2C2VmWzmnmbRfM59Ils7j1oR21LktE5CSVzCr5NWBVqCVWrjq7k5t+upFD/TlmZlO1LkdEZMS0vHOyEmfNagTguQP9Na5ERORECu5xzG/LArBTwS0iMaPgHsdZYXA/t/9YjSsRETmRgnscM7MpWhqS6nGLSOwouMuY35bluQPqcYtIvCi4y1jS3sSzvUdqXYaIyAkU3GUs72xhx/5++ofytS5FRGSEgruMFWc04w6be9TrFpH4UHCXsbyzBYB12w/UuBIRkeMqWmRqulo4u4n25gx/+5Mn+cw9T5PNJOhaOItPvflcOmc01Lo8EZmmFNxlJALjjr94DT9av4sD/UMcOpbjZ4/vpufwALe8/1LSSX1gEZHqU3BPYMHsLH951bKR569fvouP3Laed379d7x2WQfpZEA6GbBwVpbXr+gglVCYi0i0FNyn6Prz5zGYK/K5XzzN537x9AmvXXn2HFa/+1UkFd4iEiEF92n4w4vO4g8vOotC0RnKFxnMF/hB905u+ulGfvLY87ztgvm1LlFEpjB1DV+CRGA0phO0ZtO8/7WLmd/WyB3rdnF0MM9gvlDr8kRkilKPe5KYGW9+5Zl89YFnefmNdwPQ3pzmTy9frF10RGRSKbgn0Z+/7mXMackwmC+SLxR5eNsB/vnnm1g+p4Wrz+2sdXkiMkVUsufkt4A3AT3uvjL6kurXzGyKP7ls8cjzoXyRa7/wIF+6f7OCW0QmTSVj3N8Brom4jikpnQx46/nzePS5g/QcHqh1OSIyRVSy5+SDZrYo+lKmpqvO6eQz9z7Nf/zab5nZmGJmY4o3vvwM3nnxAhLBtNvKU0QmgWaVROycuS382eWLWTanmbZsmp6+Qf77jzbwlfs317o0EalT5u4Tn1Tqcd9VbozbzFYBqwAWLFjwqu3bt09SiVOLu/OBW9bxsw27WTQ7y5tfeSaBGelkQCYZcO6ZM7h7w252HRzg6+95FWbqlYtMB2a21t27Kjp3soJ7tK6uLu/u7q7k1GnpUH+Of3nwWVY/uIV8cfz2f/elC5nb2sBQvkgyKIX72WfM4PKl7Tzy3EEuOKuVQMMtIlPCqQS3pgPWwMxsio9dczbXrpxLQyrgZR3N5IpFjg0VuPWh5/jNs3tpaUhy85rtlPt39e/fupJ3X7qweoWLSCxM2OM2s1uBK4B2YA9wo7t/s9yfUY97chwdzJMIjHQioODOsVyBb/9628gaKbOb0nzsmhWkkwGpREBHc4YLF7ZpoSuROjSpPW53/6OXXpKcjqbM8V9PgJFKBHz4yqWkkka+4Pyv+zbzNz98/IQ/c968mXz3Ty9mVlO62uWKSJVUNMZ9qtTjro6BXIHew4Pki06uUOSxnYf4b3c+jgH/4cL5vOuSBWTTCVKJgFlN6RP+IRCReNEY9zTRkEpw1qzsyPPlnS3s2HeUL963mVsf2sGtD+0YeS0RGJcsnkVDKkFHc4ZU0kgGAamEMa+1ka5Fs3ho637e8+qFWpZWJOYU3FPMh65cxrlnzmDpnGY27T5CrlBkKF9k676j3PPEblKJgA27Do300nOFIgO54sif37DrEBcsbCOTCEgmjFlNaS5ZPJvGdKKGfysRGU1DJcK6HQf43L1P03t4kKd2Hz7p9dZsire88kxmN2VIJwMSQWn8PZcv8qtn9vKhK5dyztwZFN0pOgQGDckEQWCs3b6f1myal3U01+BvJlI/Jn0e96lScNevY0MFDg/kGCoUyRecHfv7ufl323nwmd4TeuYTMYOWTJK+gTyzm9L8/VtXkg6HYDKpgEwywfy2Rs5sbYzqryJSVxTcEol8oUi+6OSLTt+xHE/t7mNJezP/79m99B3LExgEZhTcOTKQ5+4ndtM/VODIYJ5Dx3Jj/syrz+nkokVt4Z2jCZobkly6eBYdLRndNSrTioJbYsHdyRVKY+mb9hwmYYYZDOaLDOQKdG87wNd/tYX+oZN3CxpekCuZKM1jzyQDzj+rlXdespCZjamRTZqzqYTuHpUpQcEtdaNQdAZyBYbyRYYKRXr6BlmzdR/b9h3lyECeXNHJF4ocGczz8LYDDOVPHK5pb07zwd9bymuXtRNYaaZMe0uabFrX3aW+aDqg1I1EYDRlkjRlSs87ZzRw3vyZY5676+AxfvvsvlLI5wsMFYo8sKmXv/vJkyf9zBWdLaSTAYGVnmeSCZoypTntqURAIjAaUgFzZzaSDmfQJBMB6YQxkCuybscB3n3pQjpnNJzws2dmU7RkkhrGkZpSj1vqWrHoPPhML30DeYrhFMcte4/y1At95ItemulShIF8gf7BArlwnL5Q9LJj7+W0ZlOcN28mycBIBAHJwOickeGype3Mbs6Ex41sujTPXksQSCXU45ZpIwiMK1bMOe0/P5ArlC64FkpDNYO5Is8fPMai9ibWbj/A0cH8yLlOaWXHjbv7eLb3KMXwQm2hWORXz/Ty3d+evJRxMjBmNKZIBhb29o2GVIJz5pZWeUwnS8GfTATMa21keWezboCSCSm4ZVprSJ18Y9Hw3ajXnTe34p8zmC+wfsdBBvJFCsXSVMrDA3m27D1C37F86UaocIpl/1CeX27cw52P7Drp56QSpYBPmBEERmAwuznDtSvPYHlnC6lEaRw/m0mworOF1mxaOylNQwpukUmQSSa4ZMnsis8fyBV44dAAhWJxZObN1r1H2bT7MLlCkUIRiu7ki0W27+vnS/eNvWPS8Hz5dDJBKlFas33ZnBYuXtxGJpkgERjJoHQH7FChyJL25pGx/8CMwIzGdIK2bEo9/Tqi4BapgYZUgsXtTScce8X81nHP39M3QN+xHLlCKcwP9ufY3HOEg8dyHOofGpl9cyxXZN32A/xi455Trqklk6QxXJQsCCBhRktDio6WDB3NGZobkiQDI5NK0N6cZlZTmoZkYmTKZioZlKZuhjdYpcLjyURANp0Y89ONnB4Ft0gd6JzRcNIMl9ct7xjzXHenbyBPIQzzXNHp6RsgERjP7T9GwR13x700HbN/KM++o0Mc7M8xkCuQK5Qu6haKTt9Ajj19A2zYdYijg3nyRWcwX/kdtKO1ZVMjs3qGh4QaUgkaU6XwT4TXAZKBcWZrIwO5AufNnxkun3D8E0JrNkVgxpyWDGaMbDaSSgTMaEzRlk1N+Vk/Cm6RKcbMmNmYOuHYvHBpgXK9+koVis6B/iH2Hx0amX+fy5eGfAbzpTn5g8PHw9f6BvLhEsRFhvKjFzgrcCycxz/8qSFXKPLA072kEwG3PfzcKdc3POZfCLcFzCSDkRk+janS8FFgRjJhIzd6ZZKlTwap8EJxNp1gVlMaM0gEpeOpcNpoWzbNglnZkWsQgRmNVb4RTMEtIqckERjtzRnamzORvYeHC5a9cOgYxXC8v+BOsejsPTKEu7P36BDDUWkGg7kiB4/l2HdkMOydl14bzJdu4Nq+r3/kH5OiQ26gdF3hYH+OoXyRfHi94XQkAqMtm2LBrCx3fOCyyWmEMioKbjO7BvgCkAC+4e7/GGlVIjKtmRkJg/lt2ZNeW9YZ3ft6OER0eKA0x9/hhAvIuYKzp2+Anr4BCl46P18src2z7+gQ1ep0TxjcZpYAvgy8AdgJPGxmP3b3J8v/SRGR+mLhEEpbU5q2GG//V8n8n4uBze6+xd2HgNuA66MtS0RExlNJcM8DRl8h2BkeO4GZrTKzbjPr7u3tnaz6RETkRSoJ7rFGbU4awXf31e7e5e5dHR1jT1MSEZGXrpLg3gmcNer5fOD5aMoREZGJVBLcDwPLzGyxmaWBG4AfR1uWiIiMZ8JZJe6eN7MPAXdTmg74LXd/IvLKRERkTBXN43b3nwI/jbgWERGpgJYDExGpM5HsgGNmvcDJq8pXph3YO4nlTJa41gXxrS2udUF8a4trXRDf2uJaF5xabQvdvaIpeZEE90thZt2Vbt9TTXGtC+JbW1zrgvjWFte6IL61xbUuiK42DZWIiNQZBbeISJ2JY3CvrnUB44hrXRDf2uJaF8S3trjWBfGtLa51QUS1xW6MW0REyotjj1tERMqITXCb2TVmtsnMNpvZx2NQzzYze9zM1ptZd3hslpnda2bPhN/bqlTLt8ysx8w2jDo2Zi1W8sWwHR8zswurXNffmtmusN3Wm9l1o177RFjXJjP7/QjrOsvM7jezjWb2hJl9JDwehzYbr7aatpuZNZjZQ2b2aFjX34XHF5vZmrDNvh8ue4GZZcLnm8PXF0VR1wS1fcfMto5qs/PD41X7fYbvlzCzR8zsrvB59G3mIxuH1u6L0q30zwJLgDTwKHBujWvaBrS/6Ng/Ax8PH38c+Kcq1fI64EJgw0S1ANcBP6O0quOlwJoq1/W3wH8Z49xzw99rBlgc/r4TEdU1F7gwfNwCPB2+fxzabLzaatpu4d+9OXycAtaEbfFvwA3h8a8BfxE+/gDwtfDxDcD3I2yz8Wr7DvCOMc6v2u8zfL//DHwPuCt8HnmbxaXHXS+bNVwPfDd8/F3grdV4U3d/ENhfYS3XA//bS34HtJrZ3CrWNZ7rgdvcfdDdtwKbKf3eo6jrBXdfFz4+DGyktIZ8HNpsvNrGU5V2C//uR8KnqfDLgSuB28PjL26z4ba8HbjKLJqt1cvUNp6q/T7NbD7wB8A3wudGFdosLsFd0WYNVebAPWa21sxWhcc63f0FKP0PCMypWXXj1xKHtvxQ+BH1W6OGk2pSV/hx9AJKvbRYtdmLaoMat1v4kX890APcS6l3f9Dd82O890hd4euHgNlR1DVWbe4+3GY3hW32OTMb3r24mr/PzwMfA4rh89lUoc3iEtwVbdZQZZe5+4XAtcAHzex1Na6nUrVuy68CLwPOB14APhMer3pdZtYM/BD4qLv3lTt1jGPVrq3m7ebuBXc/n9Ka+xcD55R576q22YtrM7OVwCeAs4GLgFnA31SzNjN7E9Dj7mtHHy7z3pNWV1yCO3abNbj78+H3HuBOSv8h7xn+yBV+76ldhePWUtO2dPc94f9kReDrHP9YX9W6zCxFKRhvcfc7wsOxaLOxaotLu4W1HAQeoDQ+3Gpmw6uIjn7vkbrC12dS+bDZZNR2TTjs5O4+CHyb6rfZZcBbzGwbpeHdKyn1wCNvs7gEd6w2azCzJjNrGX4MvBHYENb03vC09wL/XpsKoUwtPwbeE15ZvxQ4NDw8UA0vGkt8G6V2G67rhvDK+mJgGfBQRDUY8E1go7t/dtRLNW+z8WqrdbuZWYeZtYaPG4GrKY2/3w+8IzztxW023JbvAO7z8KpblWp7atQ/wkZpHHl0m0X++3T3T7j7fHdfRCmz7nP3d1GNNoviKuvpfFG6Evw0pXG1T9a4liWUruQ/CjwxXA+l8ahfAs+E32dVqZ5bKX18zlH6V/vPxquF0sexL4ft+DjQVeW6/jV838fC/1Dnjjr/k2Fdm4BrI6zrckofQR8D1odf18WkzcarrabtBrwCeCR8/w3Ap0b9v/AQpYuiPwAy4fGG8Pnm8PUlEbbZeLXdF7bZBuBmjs88qdrvc1SNV3B8VknkbaY7J0VE6kxchkpERKRCCm4RkTqj4BYRqTMKbhGROqPgFhGpMwpuEZE6o+AWEakzCm4RkTrz/wF9fCUkCimgwAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f3cf14f37f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(plt_data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plot ought to show a pretty successful learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nnPredict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to \"reverse\" the process and use our newly optimized weights to predict a label given a feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nnPredict(w1, w2, data):\n",
    "    \"\"\"% nnPredict predicts the label of data given the parameter w1, w2 of Neural\n",
    "    % Network.\n",
    "\n",
    "    % Input:\n",
    "    % w1: matrix of weights of connections from input layer to hidden layers.\n",
    "    %     w1(i, j) represents the weight of connection from unit i in input \n",
    "    %     layer to unit j in hidden layer.\n",
    "    % w2: matrix of weights of connections from hidden layer to output layers.\n",
    "    %     w2(i, j) represents the weight of connection from unit i in input \n",
    "    %     layer to unit j in hidden layer.\n",
    "    % data: matrix of data. Each row of this matrix represents the feature \n",
    "    %       vector of a particular image\n",
    "       \n",
    "    % Output: \n",
    "    % label: a column vector of predicted labels\"\"\"\n",
    "\n",
    "    labels = np.array([])\n",
    "    bias = np.ones((len(data), 1))\n",
    "\n",
    "    # Forward Propagation\n",
    "    X = np.append(data, bias, 1) # append bias\n",
    "    net1 = X.dot(w1.T)\n",
    "    o1 = sigmoid(net1)\n",
    "    \n",
    "    H = np.append(o1, bias, 1)\n",
    "    net2 = H.dot(w2.T)\n",
    "    o2 = sigmoid(net2)\n",
    "    \n",
    "    labels = np.array(np.argmax(o2, axis=1))\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = nn_params.x[0:n_hidden * (n_input + 1)].reshape((n_hidden, (n_input + 1)))\n",
    "w2 = nn_params.x[(n_hidden * (n_input + 1)):].reshape((n_class, (n_hidden + 1)))\n",
    "\n",
    "# Test the computed parameters\n",
    "\n",
    "predicted_label = nnPredict(w1, w2, train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 2 4 ... 7 0 8]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6. 2. 4. ... 7. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 93.43 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy:\", sum(predicted_label == train_label)/len(train_label)*100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
